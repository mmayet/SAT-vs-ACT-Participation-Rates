{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the data and perform basic operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Load the data in using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#sns.set_style('whitegrid')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "act_data = pd.read_csv(\"./data/act.csv\", na_values=['NA'])\n",
    "sat_data = pd.read_csv(\"./data/sat.csv\", na_values=['NA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Print the first ten rows of each dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_data.drop(columns=\"Unnamed: 0\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_data.drop(columns=\"Unnamed: 0\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Describe in words what each variable (column) is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Unnamed: Id associated to the row\n",
    "\n",
    "State: Location of the data in the row\n",
    "\n",
    "Participation: Percent of students that took the test\n",
    "\n",
    "Subjects: Average/Mean of students' scores for that subject\n",
    "\n",
    "Composite: Mean of ACT subjects\n",
    "\n",
    "Total: Sum of all subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Does the data look complete? Are there any obvious issues with the observations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The first column of both files of data are missing a name. It's currently entitled \"Unnamed: 0\".\n",
    "\n",
    "We'll need to adjust Unnamed: 0 and Math since they're both shared features in ACT and SAT.\n",
    "\n",
    "The Math score for Maryland can't be 52 (based on the total, and the actual source of the data). It should actually be 524."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_data.loc[sat_data[\"Math\"] == 52] = 524"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Print the types of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State             object\n",
      "Participation     object\n",
      "English          float64\n",
      "Math             float64\n",
      "Reading          float64\n",
      "Science          float64\n",
      "Composite        float64\n",
      "dtype: object\n",
      "State                                 object\n",
      "Participation                         object\n",
      "Evidence-Based Reading and Writing     int64\n",
      "Math                                   int64\n",
      "Total                                  int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(act_data.dtypes)\n",
    "print(sat_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Do any types need to be reassigned? If so, go ahead and do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There are two things based on preference:\n",
    "1. We can convert the Participation column into a float\n",
    "2. We can match the Total column within the SAT data to the Total column within the ACT data by converting it to a float.\n",
    "   \n",
    "   The same can be done to all of the subject values within the SAT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_data[\"Participation\"] = act_data[\"Participation\"].str.rstrip(\"%\").astype('float')\n",
    "sat_data[\"Participation\"] = sat_data[\"Participation\"].str.rstrip(\"%\").astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two issues have been addressed and fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. Create a dictionary for each column mapping the State to its respective value for that column. (For example, you should have three SAT dictionaries.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def each_val_per_state(data):\n",
    "    state_dict = {}\n",
    "    \n",
    "    data_columns = list(data)\n",
    "    state_loc = data_columns.index(\"State\")+1\n",
    "    \n",
    "    for x in data_columns[state_loc:]:\n",
    "        state_dict[x] = data.set_index(\"State\").to_dict()[x]\n",
    "\n",
    "    return state_dict\n",
    "\n",
    "sat_dict = each_val_per_state(sat_data)\n",
    "act_dict = each_val_per_state(act_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Create one dictionary where each key is the column name, and each value is an iterable (a list or a Pandas Series) of all the values in that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_data_list = sat_data.to_dict('list')\n",
    "sat_data_series = sat_data.to_dict('series')\n",
    "act_data_list = act_data.to_dict('list')\n",
    "act_data_series = act_data.to_dict('series')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. Merge the dataframes on the state column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataframe = pd.merge(act_data,sat_data, on=[\"State\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. Change the names of the columns so you can distinguish between the SAT columns and the ACT columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataframe = pd.merge(act_data,sat_data, on=[\"State\"], suffixes=('_act','_sat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11. Print the minimum and maximum of each numeric column in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only run once\n",
    "rearranged_columns = ['State', 'Participation_act', 'English', 'Math_act', 'Reading', 'Science', 'Composite', 'Participation_sat', 'Evidence-Based Reading and Writing', 'Math_sat', 'Total']\n",
    "merged_dataframe = merged_dataframe[rearranged_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: \n",
      " Math_act                                18\n",
      "Reading                               18.1\n",
      "Science                               18.2\n",
      "Composite                             17.8\n",
      "Participation_sat                        2\n",
      "Evidence-Based Reading and Writing     482\n",
      "Math_sat                               468\n",
      "Total                                  950\n",
      "dtype: object \n",
      "\n",
      " Max: \n",
      " Math_act                              25.3\n",
      "Reading                                 26\n",
      "Science                               24.9\n",
      "Composite                             25.5\n",
      "Participation_sat                      100\n",
      "Evidence-Based Reading and Writing     644\n",
      "Math_sat                               651\n",
      "Total                                 1295\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "a = merged_dataframe.min()[3:]\n",
    "b = merged_dataframe.max()[3:]\n",
    "print(\"Min:\",\"\\n\", a,\"\\n\\n\", \"Max:\",\"\\n\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12. Write a function using only list comprehensions, no loops, to compute standard deviation. Using this function, calculate the standard deviation of each numeric column in both data sets. Add these to a list called `sd`.\n",
    "\n",
    "$$\\sigma = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dataframe[\"English\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_std(df):\n",
    "    return [np.sum(np.square(df[x] - df[x].mean()))/df[x].count() ** .5 for x in df.columns[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1004.72,\n",
       "  5.425344,\n",
       "  3.8532640000000016,\n",
       "  4.1761,\n",
       "  2.9629439999999994,\n",
       "  3.9949159999999995,\n",
       "  1227.0516,\n",
       "  2063.0916,\n",
       "  2198.3684000000003,\n",
       "  8466.1636],\n",
       " [32.019126936815724,\n",
       "  2.3528844651028757,\n",
       "  1.9829024284659917,\n",
       "  2.064298072133054,\n",
       "  1.7387962056831037,\n",
       "  2.019020777000372,\n",
       "  35.38493280179792,\n",
       "  45.8824095945721,\n",
       "  47.3627813924023,\n",
       "  92.94591141615425])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa = [np.sum(np.square(merged_dataframe[x] - merged_dataframe[x].mean()))/merged_dataframe[x].count() for x in merged_dataframe.columns[1:]]\n",
    "bbb = [merged_dataframe[x].std() for x in rearranged_columns[1:]]\n",
    "\n",
    "aaa, bbb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filler_columns_mean = [\"Mean\"] #not asked for, but useful to see\n",
    "filler_columns_sd = [\"Std\"]\n",
    "\n",
    "mean = [merged_dataframe[x].mean() for x in rearranged_columns[1:]]\n",
    "sd = [merged_dataframe[x].std() for x in rearranged_columns[1:]]\n",
    "\n",
    "mean = pd.Series(filler_columns_mean+mean, name=\"Mean\", index=rearranged_columns)\n",
    "sd = pd.Series(filler_columns_sd+sd, name=\"Sd\", index=rearranged_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Manipulate the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 13. Turn the list `sd` into a new observation in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataframe.loc[\"Mean\"] = mean\n",
    "merged_dataframe.loc[\"Std\"] = sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 14. Sort the dataframe by the values in a numeric column (e.g. observations descending by SAT participation rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataframe.sort_values(\"Participation_act\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 15. Use a boolean filter to display only observations with a score above a certain threshold (e.g. only states with a participation rate above 50%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataframe.query(\"Participation_sat > 50\").tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 16. Using MatPlotLib and PyPlot, plot the distribution of the Rate columns for both SAT and ACT using histograms. (You should have two histograms. You might find [this link](https://matplotlib.org/users/pyplot_tutorial.html#working-with-multiple-figures-and-axes) helpful in organizing one plot above the other.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sns_dist_plot(data, title, color):\n",
    "    f, ax = plt.subplots(1, 1, figsize=(10,5), sharey=True)\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    sns.distplot(data[:50],\n",
    "                 kde_kws={\"color\": \"k\", \"lw\": 3, \"label\": \"KDE\"},\n",
    "                 hist_kws={\"histtype\": \"step\", \"linewidth\": 3, \"alpha\": 1, \"color\": color}, ax=ax)\n",
    "    #plt.title(title)\n",
    "    plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_dist_plot(merged_dataframe[\"Participation_act\"], \"% of ACT Participation\", \"g\")\n",
    "sns_dist_plot(merged_dataframe[\"Participation_sat\"], \"% of SAT Participation\", \"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 17. Plot the Math(s) distributions from both data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns_dist_plot(merged_dataframe[\"Math_act\"], \"ACT Math Scores\", \"g\")\n",
    "sns_dist_plot(merged_dataframe[\"Math_sat\"], \"SAT Math Scores\", \"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributions from both tests' math scores are quite similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 18. Plot the Verbal distributions from both data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#sns_plot(merged_dataframe[\"English\"], \"ACT English Scores\", \"g\")\n",
    "\n",
    "f, (ax1,ax2) = plt.subplots(1, 2, figsize=(15,5), sharey=True)\n",
    "ax1.set_title('ACT English Scores', fontsize=20)\n",
    "sns.distplot(merged_dataframe[\"English\"][:50],\n",
    "             kde_kws={\"color\": \"k\", \"lw\": 3, \"label\": \"KDE\"},\n",
    "             hist_kws={\"histtype\": \"step\", \"linewidth\": 3, \"alpha\": 1, \"color\": \"y\"}, ax=ax1)\n",
    "ax2.set_title('ACT Reading Scores', fontsize=20)\n",
    "sns.distplot(merged_dataframe[\"Reading\"][:50],\n",
    "             kde_kws={\"color\": \"k\", \"lw\": 3, \"label\": \"KDE\"},\n",
    "             hist_kws={\"histtype\": \"step\", \"linewidth\": 3, \"alpha\": 1, \"color\": \"r\"}, ax=ax2)\n",
    "plt.figure()\n",
    "sns_dist_plot(merged_dataframe[\"Evidence-Based Reading and Writing\"], \"SAT Evidence-Based Reading and Writing Scores\", \"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributions of ACT's English and Reading scores are very similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19. When we make assumptions about how data are distributed, what is the most common assumption?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20. Does this assumption hold true for any of our columns? Which?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the following code snippet, we can look at each individual column to assess if it's a normal distribution or not.\n",
    "\n",
    "`for x in rearranged_columns[1:]:\n",
    "    sns_dist_plot(merged_dataframe[x], x, \"b\")`\n",
    "   \n",
    "After looking at all of the histograms, none of them are normal distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21. Plot some scatterplots examining relationships between all variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to look at scatter plots comparing the relationship between all our variables, we could run the following bit of code:\n",
    "\n",
    "`f,ax = plt.subplots(10,10, figsize=(20,20))\n",
    "for x,c1 in enumerate(rearranged_columns[1:]):\n",
    "    for y,c2 in enumerate(rearranged_columns[1:]):\n",
    "        sns.regplot(merged_dataframe[c1],merged_dataframe[c2],ax=ax[x,y])`\n",
    "        \n",
    "However, that's extremely CPU intensive, and we can suffice with looking the few I've highlighted, graphed, and commented on below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sns_reg_plot(dataX, dataY, title):\n",
    "    f, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    sns.regplot(dataX[:50], dataY[:50], ax=ax)\n",
    "    plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns_reg_plot(merged_dataframe[\"Participation_act\"],merged_dataframe[\"Composite\"],\"ACT Participation vs Composite Score\")\n",
    "sns_reg_plot(merged_dataframe[\"Participation_sat\"],merged_dataframe[\"Total\"],\"SAT Participation vs Total Score\")\n",
    "sns_reg_plot(merged_dataframe[\"Participation_act\"],merged_dataframe[\"Participation_sat\"],\"ACT vs SAT Participation\")\n",
    "sns_reg_plot(merged_dataframe[\"Participation_act\"],merged_dataframe[\"Total\"],\"ACT Part vs SAT Scores\")\n",
    "sns_reg_plot(merged_dataframe[\"Participation_sat\"],merged_dataframe[\"Composite\"],\"SAT Part vs ACT Scores\")\n",
    "sns_reg_plot(merged_dataframe[\"Composite\"],merged_dataframe[\"Total\"],\"ACT vs SAT Scores\")\n",
    "# sns_reg_plot(merged_dataframe[\"Total\"],merged_dataframe[\"Composite\"],\"SAT vs ACT Scores\")\n",
    "# sns_reg_plot(merged_dataframe[\"Math_act\"],merged_dataframe[\"Science\"],\"ACT: Math & Science\")\n",
    "# sns_reg_plot(merged_dataframe[\"English\"],merged_dataframe[\"Reading\"],\"ACT: English & Reading\")\n",
    "# sns_reg_plot(merged_dataframe[\"Reading\"]+merged_dataframe[\"English\"],merged_dataframe[\"Math_act\"]+merged_dataframe[\"Science\"],\"ACT: Math+Science & English+Reading\")\n",
    "# sns_reg_plot(merged_dataframe[\"Evidence-Based Reading and Writing\"],merged_dataframe[\"Math_sat\"],\"ACT: Math & Science\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22. Are there any interesting relationships to note?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key things to note are the following:\n",
    "- Test participation (ACT and SAT) is negatively correlated to total test score\n",
    "- ACT participation is negatively correlated to SAT participation\n",
    "\n",
    "Based on that, we'll later look into these six states based on their interesting participation rates:\n",
    "1. Mississippi:\n",
    "    - ACT: 100% - 2nd Lowest ACT Score\n",
    "    - SAT: 2%   - 9th Highest SAT Score\n",
    "2. Maine\n",
    "    - ACT: 8%   - 4rth Highest ACT Score\n",
    "    - SAT: 95%  - 5th Lowest SAT\n",
    "3. Nevada\n",
    "    - ACT: 100%  - Lowest ACT Score\n",
    "    - SAT: 26%   - Average SAT Score\n",
    "4. Delaware\n",
    "    - ACT: 18%  - 8th Highest ACT Score\n",
    "    - SAT: 100% - 2nd Lowest SAT Score\n",
    "5. DC\n",
    "    - ACT: 32%  - 5th Highest ACT Score\n",
    "    - SAT: 100% - Lowest SAT Score\n",
    "6. New Hampshire\n",
    "    - ACT: 18%  - Highest ACT Score\n",
    "    - SAT: 96%  - 12th Lowest SAT Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23. Create box plots for each variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(5,2,figsize=(20,17));\n",
    "\n",
    "for ax, var in zip(axes.flatten(), rearranged_columns[1:]):\n",
    "    ax.set_title(var, fontsize=13);\n",
    "    sns.boxplot(data=merged_dataframe[var][:50], orient=\"h\", ax=ax);\n",
    "    plt.tight_layout();\n",
    "    plt.figure();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BONUS: Using Tableau, create a heat map for each variable using a map of the US. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://us-west-2b.online.tableau.com/#/site/mayetm93atgmailcom/workbooks/51742/views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Descriptive and Inferential Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24. Summarize each distribution. As data scientists, be sure to back up these summaries with statistics. (Hint: What are the three things we care about when describing distributions?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2,5,figsize=(20,10))\n",
    "\n",
    "for ax, var in zip(axes.flatten(), rearranged_columns[1:]):\n",
    "    sns.distplot(merged_dataframe[var][:50],\n",
    "             kde_kws={\"color\": \"k\", \"lw\": 3, \"label\": \"KDE\"},\n",
    "             hist_kws={\"histtype\": \"step\", \"linewidth\": 3, \"alpha\": 1, \"color\": \"b\"}, ax=ax)\n",
    "    print('{}\\'s mean is: {}, and it\\'s std is: {}'.format(var,merged_dataframe[var][50],merged_dataframe[var][51]))\n",
    "#\"Mean = \" + str(np.mean(merged_dataframe[var][:50])) + \"\\n Std = \" + str(np.std(merged_dataframe[var][:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of them are normally distributed. They're very bi-modal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2,5,figsize=(20,10))\n",
    "\n",
    "for ax, var in zip(axes.flatten(), rearranged_columns[1:]):\n",
    "    sns.distplot(merged_dataframe.query(\"Participation_act < 90 and Participation_act > 10 and Participation_sat < 90 and Participation_sat > 10\")[var][:50],\n",
    "             kde_kws={\"color\": \"k\", \"lw\": 3, \"label\": \"KDE\"},\n",
    "             hist_kws={\"histtype\": \"step\", \"linewidth\": 3, \"alpha\": 1, \"color\": \"b\"}, ax=ax)\n",
    "    print('{}\\'s mean is: {}, and it\\'s std is: {}'.format(var,merged_dataframe[var][50],merged_dataframe[var][51]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if take out states that have very high or very low participation rates, none of them are normally distributed. However, most tend to have a somewhat normal peak, with a few outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25. Summarize each relationship. Be sure to back up these summaries with statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, when we look at any scores, seeing subject scores for each test are positively correlated. See `sns_reg_plot(merged_dataframe[\"Reading\"]+merged_dataframe[\"English\"],merged_dataframe[\"Math_act\"]+merged_dataframe[\"Science\"],\"ACT: Math+Science & English+Reading\")` and similar such comparisons. There's nothing surprising there.\n",
    "\n",
    "Looking back at question 21, we clearly see that participation and scores (of the same test) are negatively correlated. At the same time, participation between ACT and SAT are also negatively correlated. We also see that ACT and SAT scores are negatively correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataframe[\"Participation_sat\"][50] - merged_dataframe[\"Participation_act\"][50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26. Execute a hypothesis test comparing the SAT and ACT participation rates. Use $\\alpha = 0.05$. Be sure to interpret your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two hypotheses:\n",
    "\n",
    "$H_0$ The difference in SAT and ACT participation rates is 0.\n",
    "\n",
    "$H_1$ The difference in SAT and ACT participation rates is not 0.\n",
    "\n",
    "Our measured difference is `-26.78` (after having adjusted SAT scores by dividing by 50 to keep a similar scale).\n",
    "\n",
    "Now, `-26.78` does seem quite small, so it probably doesn't support $H_0$. But let's verify that with an $\\alpha = 0.05$.\n",
    "\n",
    "Let's go ahead an calculate our t and p values and compare that to our $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat = np.array(merged_dataframe[\"Participation_sat\"][:50])\n",
    "act = np.array(merged_dataframe[\"Participation_act\"][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvalue, pvalue = sp.stats.ttest_ind(sat, act)\n",
    "pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find a `pvalue` of $.0001$ which is much smaller than our $\\alpha$ which means we **do** have enough evidence to reject our $H_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27. Generate and interpret 95% confidence intervals for SAT and ACT participation rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_means(distribution, sample_size = 200, num_samples = 200, replace = False): # randomly sample\n",
    "    sample_means = []\n",
    "    for _ in range(num_samples):\n",
    "        this_mean = np.random.choice(distribution, size=sample_size, replace=replace).mean()\n",
    "        sample_means.append(this_mean)\n",
    "    \n",
    "    return sample_means\n",
    "\n",
    "def confidence_interval_mean(distribution, confidence=.95):\n",
    "    #means = np.mean(distribution)\n",
    "    means = get_sample_means(distribution, replace=True)\n",
    "    \n",
    "    dist_from_0_or_100 = (100-confidence)/2\n",
    "    lower_percentile, upper_percentile = 0+dist_from_0_or_100, 100-dist_from_0_or_100\n",
    "\n",
    "    return (np.percentile(means, lower_percentile), np.percentile(means, upper_percentile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cim_act = confidence_interval_mean(merged_dataframe[\"Participation_act\"][:50])\n",
    "cim_sat = confidence_interval_mean(merged_dataframe[\"Participation_sat\"][:50])\n",
    "cim_act, cim_sat"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The confidence interval for ACT partication rates gives a range between 66.06 and 66.18, and the confidence interval for SAT partication rates has a range between 38.88 amd 38.94.\n",
    "\n",
    "Which means that, if we were to get 100 samples of particiapten rates and re-construct the confidence internvals, then we can expect that 95% of those intervals would be have the actual mean of participation rates between those given ranges.\n",
    "\n",
    "This tells us that we can be 95% confident that the mean of the actual participation rates is between those ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 28. Given your answer to 26, was your answer to 27 surprising? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, it wasn't surprising. The reason being that, which such a small pvalue, it's quite normal to expect a very tight restrictive range for our confidence intervals. Our CI are very constrained, varying by less than a 10th of a decimal. Because of that, it's not surprising at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 29. Is it appropriate to generate correlation between SAT and ACT math scores? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_reg_plot(merged_dataframe[\"Math_act\"],merged_dataframe[\"Math_sat\"], \"Comparing Math Scores from the ACT and SAT\")\n",
    "sns_reg_plot(merged_dataframe[\"Math_act\"],merged_dataframe[\"Math_sat\"]/26, \"Comparing Math Scores from the ACT and SAT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(merged_dataframe[\"Math_act\"][:50],label=\"ACT\")\n",
    "sns.distplot(merged_dataframe[\"Math_sat\"][:50]/26,label=\"SAT\")\n",
    "plt.title(\"SAT & ACT Math Scores\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(merged_dataframe[\"Math_act\"][:50],merged_dataframe[\"Math_sat\"][:50]/26)[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these scatter plots (the later adjusts SAT scores by 26) show that there's a negative correlation between the math scores on the SAT and ACT. We can also compare their distributions.\n",
    "\n",
    "However, looking and comparing these two values doesn't make much sense. Yes, they have a correlation of `-0.41` but that doesn't give us much insight.\n",
    "\n",
    "The more appropriate correlation to look at and analyze (as done above) is with the overall scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 30. Suppose we only seek to understand the relationship between SAT and ACT data in 2017. Does it make sense to conduct statistical inference given the data we have? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two things we need to consider:\n",
    "\n",
    "1. Are these numbers inconsistent in comparison to previous years?\n",
    "2. Are there going to be changes to policy in future years without having considered the 2017 data?\n",
    "\n",
    "In both cases, if the answer is yes, then we absolutely do not want to use this data to infer. Because we are unsure about data from previous years and the trends and relationships contained therein. And also because, if there is already policy enacted for future years that undermine this data, there's no purpose to infer.\n",
    "\n",
    "However, if this data is consistent, and it can be used to affect future policy before it's written, then we can use this data (although would be much better to look at this data along side previous years). Since the trends of 2017 would be similar to previous years, SAT and ACT policy makers could implement changes based on this data set. For example, changing mandatory participation, waiving fees, or setting a passing threshold.\n",
    "\n",
    "But since we know that the SAT was again redesigned to fit with Common Core standards, it seems best to hold off until enough data is collected on the new assessments' results."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
